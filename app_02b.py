import streamlit as st
import pandas as pd
from docx import Document
from langchain_community.document_loaders import (
    PyPDFLoader, TextLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
import tempfile
from langchain_google_genai import GoogleGenerativeAIEmbeddings
import google.generativeai as genai
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains.question_answering import load_qa_chain # V·∫´n import ƒë·ªÉ l·∫•y PromptTemplate g·ªëc
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
import time # Import th√™m th∆∞ vi·ªán time ƒë·ªÉ c√≥ th·ªÉ th√™m ƒë·ªô tr·ªÖ n·∫øu c·∫ßn

# Load environment variables
load_dotenv()
api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    st.error("GOOGLE_API_KEY not found in environment variables.")
    st.stop()
genai.configure(api_key=api_key)

def get_text_from_documents(docs):
    """Extract text from uploaded documents (PDF, TXT, XLSX, CSV, DOCX)."""
    text = ""
    try:
        for file in docs:
            suffix = os.path.splitext(file.name)[-1].lower()
            with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:
                tmp_file.write(file.read())
                tmp_path = tmp_file.name

            # Loaders theo lo·∫°i file
            if suffix == ".pdf":
                loader = PyPDFLoader(tmp_path)
                pages = loader.load_and_split()
                text += "\n".join([p.page_content for p in pages])

            elif suffix == ".txt":
                loader = TextLoader(tmp_path)
                pages = loader.load_and_split()
                text += "\n".join([p.page_content for p in pages])

            elif suffix == ".csv":
                df = pd.read_csv(tmp_path)
                text += df.to_string(index=False)

            elif suffix == ".xlsx":
                try:
                    sheets = pd.read_excel(tmp_path, sheet_name=None)
                    for name, df in sheets.items():
                        text += f"\n--- Sheet: {name} ---\n"
                        text += df.to_string(index=False)
                except Exception as e:
                    st.warning(f"L·ªói ƒë·ªçc Excel: {e}")
                    continue
                
            elif suffix == ".docx":
                try:
                    doc = Document(tmp_path)
                    for para in doc.paragraphs:
                        text += para.text + "\n"
                    # C√≥ th·ªÉ b·ªï sung ƒë·ªçc text t·ª´ b·∫£ng (tables) n·∫øu c·∫ßn #
                    for table in doc.tables:
                        for row in table.rows:
                            for cell in row.cells:
                                text += cell.text + " "
                        text += "\n" # Th√™m xu·ªëng d√≤ng sau m·ªói b·∫£ng #
                    text += "\n" # Th√™m xu·ªëng d√≤ng sau m·ªói file docx ƒë·ªÉ t√°ch bi·ªát #
                except Exception as e:
                    st.warning(f"L·ªói ƒë·ªçc file DOCX: {e}")
                    continue
                
            else:
                st.warning(f"Kh√¥ng h·ªó tr·ª£ ƒë·ªãnh d·∫°ng file: {suffix}")
                continue

            os.unlink(tmp_path)

    except Exception as e:
        st.error(f"L·ªói x·ª≠ l√Ω t√†i li·ªáu: {str(e)}")
        return ""
    return text

def get_text_chunks(text):
    """Split text into chunks."""
    try:
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)
        chunks = text_splitter.split_text(text)
        return chunks
    except Exception as e:
        st.error(f"Error splitting text: {str(e)}")
        return []

def get_vector_store(text_chunks):
    """Create and save FAISS vector store."""
    try:
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)
        vector_store.save_local("faiss_index")
        st.success("T√†i li·ªáu ƒë√£ ph√¢n t√≠ch xong, s·∫µn s√†ng ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.")
    except Exception as e:
        st.error(f"Error creating vector store: {str(e)}")

# ƒê√£ thay ƒë·ªïi h√†m n√†y ƒë·ªÉ tr·∫£ v·ªÅ model v√† prompt template
def get_conversational_chain_components(): # ƒê√É THAY ƒê·ªîI T√äN H√ÄM #
    """Returns the LLM model and the prompt template."""
    prompt_template = """
    Tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch chi ti·∫øt nh·∫•t c√≥ th·ªÉ d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p. N·∫øu c√¢u tr·∫£ l·ªùi kh√¥ng c√≥ trong ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p, h√£y n√≥i, "C√¢u tr·∫£ l·ªùi kh√¥ng c√≥ trong ng·ªØ c·∫£nh."
    Kh√¥ng cung c·∫•p th√¥ng tin sai l·ªách.

    Ng·ªØ c·∫£nh: {context}
    C√¢u h·ªèi: {question}

    Answer:
    """
    try:
        model = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0.7)
        prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
        return model, prompt # TR·∫¢ V·ªÄ MODEL V√Ä PROMPT TEMPLATE #
    except Exception as e:
        st.error(f"Error initializing QA chain components: {str(e)}") # ƒê√É ƒê·ªîI TH√îNG B√ÅO L·ªñI #
        return None, None

def user_input(user_question):
    """Process user question and return streamed response.""" # ƒê√É C·∫¨P NH·∫¨T M√î T·∫¢ #
    try:
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        # Load FAISS index with deserialization permission
        if not os.path.exists("faiss_index"):
            st.error("Kh√¥ng t√¨m th·∫•y FAISS index. H√£y t·∫£i t√†i li·ªáu l√™n tr∆∞·ªõc.")
            return
        new_db = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
        docs = new_db.similarity_search(user_question)

        # L·∫•y model v√† prompt template t·ª´ h√†m m·ªõi #
        model, prompt_template_obj = get_conversational_chain_components() # ƒê√É THAY ƒê·ªîI #
        if not model or not prompt_template_obj: # ƒê√É THAY ƒê·ªîI #
            return

        # N·ªëi n·ªôi dung c√°c t√†i li·ªáu t√¨m ƒë∆∞·ª£c v√†o ng·ªØ c·∫£nh #
        context = "\n\n".join([doc.page_content for doc in docs])

        # T·∫°o prompt cu·ªëi c√πng #
        final_prompt_text = prompt_template_obj.format(context=context, question=user_question) # ƒê√É THAY ƒê·ªîI #

        st.subheader("Reply:") # ƒê·ªïi st.write th√†nh st.subheader cho ti√™u ƒë·ªÅ r√µ r√†ng h∆°n
        response_placeholder = st.empty() # T·∫°o m·ªôt placeholder ƒë·ªÉ c·∫≠p nh·∫≠t n·ªôi dung
        full_response_text = ""

        # Stream ph·∫£n h·ªìi t·ª´ m√¥ h√¨nh #
        for chunk in model.stream(final_prompt_text): # ƒê√É THAY ƒê·ªîI C√ÅCH G·ªåI M√î H√åNH #
            if chunk.content: # ƒê·∫£m b·∫£o chunk c√≥ n·ªôi dung ƒë·ªÉ th√™m v√†o #
                full_response_text += chunk.content
                response_placeholder.markdown(full_response_text) # C·∫≠p nh·∫≠t n·ªôi dung c·ªßa placeholder
                # T√πy ch·ªçn: Th√™m m·ªôt ƒë·ªô tr·ªÖ nh·ªè ƒë·ªÉ t·∫°o hi·ªáu ·ª©ng g√µ ph√≠m r√µ r√†ng h∆°n
                # time.sleep(0.01)

    except Exception as e:
        st.error(f"L·ªói x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}")

def main():
    st.set_page_config(page_title="Chat with Documents", page_icon="üìÑ")
    st.header("Demo Chatbot ph√¢n t√≠ch t√†i li·ªáu :robot_face:")

    user_question = st.text_input("B·∫°n h√£y h·ªèi sau khi t√†i li·ªáu ƒë√£ ƒë∆∞·ª£c ph√¢n t√≠ch xong")

    if user_question:
        user_input(user_question)

    with st.sidebar:
        st.title("Menu")
        pdf_docs = st.file_uploader(
            "T·∫£i t√†i li·ªáu l√™n (PDF, TXT, XLSX, CSV, DOCX)",
            accept_multiple_files=True,
            type=["pdf", "docx", "txt", "xlsx", "csv"]
        )
        if st.button("Ph√¢n t√≠ch t√†i li·ªáu"):
            if not pdf_docs:
                st.error("Vui l√≤ng t·∫£i t√†i li·ªáu c·ªßa b·∫°n l√™n.")
                return
            with st.spinner("ƒêang x·ª≠ l√Ω..."):
                raw_text = get_text_from_documents(pdf_docs)
                if raw_text:
                    text_chunks = get_text_chunks(raw_text)
                    if text_chunks:
                        get_vector_store(text_chunks)
                    else:
                        st.error("Ki·ªÉm tra l·∫°i n·ªôi dung trong t√†i li·ªáu.")
                else:
                    st.error("Kh√¥ng c√≥ n·ªôi dung n√†o ƒë∆∞·ª£c trong t√†i li·ªáu.")

if __name__ == "__main__":
    main()