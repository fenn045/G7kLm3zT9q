import streamlit as st
import pandas as pd
from docx import Document
from langchain_community.document_loaders import (
    PyPDFLoader, TextLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
import tempfile
from langchain_google_genai import GoogleGenerativeAIEmbeddings
import google.generativeai as genai
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv

# Load environment variables
load_dotenv()
api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    st.error("GOOGLE_API_KEY not found in environment variables.")
    st.stop()
genai.configure(api_key=api_key)

def get_text_from_documents(docs):
    """Extract text from uploaded documents (PDF, TXT, XLSX, CSV, DOCX)."""
    text = ""
    try:
        for file in docs:
            suffix = os.path.splitext(file.name)[-1].lower()
            with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:
                tmp_file.write(file.read())
                tmp_path = tmp_file.name

            # Loaders theo lo·∫°i file
            if suffix == ".pdf":
                loader = PyPDFLoader(tmp_path)
                pages = loader.load_and_split()
                text += "\n".join([p.page_content for p in pages])

            elif suffix == ".txt":
                loader = TextLoader(tmp_path)
                pages = loader.load_and_split()
                text += "\n".join([p.page_content for p in pages])

            elif suffix == ".csv":
                df = pd.read_csv(tmp_path)
                text += df.to_string(index=False)

            elif suffix == ".xlsx":
                try:
                    sheets = pd.read_excel(tmp_path, sheet_name=None)
                    for name, df in sheets.items():
                        text += f"\n--- Sheet: {name} ---\n"
                        text += df.to_string(index=False)
                except Exception as e:
                    st.warning(f"L·ªói ƒë·ªçc Excel: {e}")
                    continue
                
            elif suffix == ".docx":
                try:
                    doc = Document(tmp_path)
                    for para in doc.paragraphs:
                        text += para.text + "\n"
                    # C√≥ th·ªÉ b·ªï sung ƒë·ªçc text t·ª´ b·∫£ng (tables) n·∫øu c·∫ßn #
                    for table in doc.tables:
                        for row in table.rows:
                            for cell in row.cells:
                                text += cell.text + " "
                        text += "\n" # Th√™m xu·ªëng d√≤ng sau m·ªói b·∫£ng #
                    text += "\n" # Th√™m xu·ªëng d√≤ng sau m·ªói file docx ƒë·ªÉ t√°ch bi·ªát #
                except Exception as e:
                    st.warning(f"L·ªói ƒë·ªçc file DOCX: {e}")
                    continue
                
            else:
                st.warning(f"Kh√¥ng h·ªó tr·ª£ ƒë·ªãnh d·∫°ng file: {suffix}")
                continue

            os.unlink(tmp_path)

    except Exception as e:
        st.error(f"L·ªói x·ª≠ l√Ω t√†i li·ªáu: {str(e)}")
        return ""
    return text

def get_text_chunks(text):
    """Split text into chunks."""
    try:
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)
        chunks = text_splitter.split_text(text)
        return chunks
    except Exception as e:
        st.error(f"Error splitting text: {str(e)}")
        return []

def get_vector_store(text_chunks):
    """Create and save FAISS vector store."""
    try:
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)
        vector_store.save_local("faiss_index")
        st.success("T√†i li·ªáu ƒë√£ ph√¢n t√≠ch xong, s·∫µn s√†ng ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.")
    except Exception as e:
        st.error(f"Error creating vector store: {str(e)}")

def get_conversational_chain():
    """Create a conversational QA chain."""
    prompt_template = """
    Tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch chi ti·∫øt nh·∫•t c√≥ th·ªÉ d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p. N·∫øu c√¢u tr·∫£ l·ªùi kh√¥ng c√≥ trong ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p, h√£y n√≥i, "C√¢u tr·∫£ l·ªùi kh√¥ng c√≥ trong ng·ªØ c·∫£nh."
    Kh√¥ng cung c·∫•p th√¥ng tin sai l·ªách.

    Ng·ªØ c·∫£nh: {context}
    C√¢u h·ªèi: {question}

    Answer:
    """
    try:
        model = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0.7)
        prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
        chain = load_qa_chain(model, chain_type="stuff", prompt=prompt)
        return chain
    except Exception as e:
        st.error(f"Error initializing QA chain: {str(e)}")
        return None

def user_input(user_question):
    """Process user question and return response."""
    try:
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        # Load FAISS index with deserialization permission
        if not os.path.exists("faiss_index"):
            st.error("Kh√¥ng t√¨m th·∫•y FAISS index. H√£y t·∫£i t√†i li·ªáu PDF l√™n tr∆∞·ªõc.")
            return
        new_db = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
        docs = new_db.similarity_search(user_question)

        chain = get_conversational_chain()
        if not chain:
            return

        response = chain(
            {"input_documents": docs, "question": user_question},
            return_only_outputs=True
        )
        st.write("Reply: ", response["output_text"])
    except Exception as e:
        st.error(f"L·ªói x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}")

def main():
    st.set_page_config(page_title="Chatbot", page_icon="üìÑ")
    st.header("Demo Chatbot ph√¢n t√≠ch t√†i li·ªáu")

    user_question = st.text_input("B·∫°n h√£y h·ªèi sau khi t√†i li·ªáu ƒë√£ ƒë∆∞·ª£c ph√¢n t√≠ch xong")

    if user_question:
        user_input(user_question)

    with st.sidebar:
        st.title("Menu")
        pdf_docs = st.file_uploader(
            "T·∫£i t√†i li·ªáu l√™n (PDF, TXT, XLSX, CSV, DOCX)",
            accept_multiple_files=True,
            type=["pdf", "docx", "txt", "xlsx", "csv"]
        )
        if st.button("Ph√¢n t√≠ch t√†i li·ªáu"):
            if not pdf_docs:
                st.error("Vui l√≤ng t·∫£i t√†i li·ªáu c·ªßa b·∫°n l√™n.")
                return
            with st.spinner("ƒêang x·ª≠ l√Ω..."):
                raw_text = get_text_from_documents(pdf_docs)
                if raw_text:
                    text_chunks = get_text_chunks(raw_text)
                    if text_chunks:
                        get_vector_store(text_chunks)
                    else:
                        st.error("Ki·ªÉm tra l·∫°i n·ªôi dung trong t√†i li·ªáu")
                else:
                    st.error("Kh√¥ng c√≥ n·ªôi dung n√†o ƒë∆∞·ª£c trong t√†i li·ªáu.")

if __name__ == "__main__":
    main()