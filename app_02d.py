import streamlit as st
import pandas as pd
from docx import Document
from langchain_community.document_loaders import (
    PyPDFLoader, TextLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
import tempfile
from langchain_google_genai import GoogleGenerativeAIEmbeddings
import google.generativeai as genai
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
import json

# Load environment variables
load_dotenv()
api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    st.error("GOOGLE_API_KEY not found in environment variables.")
    st.stop()
genai.configure(api_key=api_key)

# H√†m t·∫£i l·ªãch s·ª≠ chat t·ª´ file
def load_chat_history():
    try:
        with open("rag_chat_history.json", "r", encoding="utf-8") as file:
            return json.load(file)
    except FileNotFoundError:
        return []

# H√†m l∆∞u l·ªãch s·ª≠ chat v√†o file
def save_chat_history(history):
    with open("rag_chat_history.json", "w", encoding="utf-8") as file:
        json.dump(history, file, ensure_ascii=False, indent=2)

def get_text_from_documents(docs):
    """Extract text from uploaded documents (PDF, TXT, XLSX, CSV, DOCX)."""
    text = ""
    try:
        for file in docs:
            suffix = os.path.splitext(file.name)[-1].lower()
            with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:
                tmp_file.write(file.read())
                tmp_path = tmp_file.name

            # Loaders theo lo·∫°i file
            if suffix == ".pdf":
                loader = PyPDFLoader(tmp_path)
                pages = loader.load_and_split()
                text += "\n".join([p.page_content for p in pages])

            elif suffix == ".txt":
                loader = TextLoader(tmp_path)
                pages = loader.load_and_split()
                text += "\n".join([p.page_content for p in pages])

            elif suffix == ".csv":
                df = pd.read_csv(tmp_path)
                text += df.to_string(index=False)

            elif suffix == ".xlsx":
                try:
                    sheets = pd.read_excel(tmp_path, sheet_name=None)
                    for name, df in sheets.items():
                        text += f"\n--- Sheet: {name} ---\n"
                        text += df.to_string(index=False)
                except Exception as e:
                    st.warning(f"L·ªói ƒë·ªçc Excel: {e}")
                    continue
                
            elif suffix == ".docx":
                try:
                    doc = Document(tmp_path)
                    for para in doc.paragraphs:
                        text += para.text + "\n"

                    for table in doc.tables:
                        for row in table.rows:
                            for cell in row.cells:
                                text += cell.text + " "
                        text += "\n" # Th√™m xu·ªëng d√≤ng sau m·ªói b·∫£ng
                        
                    text += "\n" # Th√™m xu·ªëng d√≤ng sau m·ªói file docx ƒë·ªÉ t√°ch bi·ªát
                except Exception as e:
                    st.warning(f"L·ªói ƒë·ªçc file DOCX: {e}")
                    continue
                
            else:
                st.warning(f"Kh√¥ng h·ªó tr·ª£ ƒë·ªãnh d·∫°ng file: {suffix}")
                continue

            os.unlink(tmp_path)

    except Exception as e:
        st.error(f"L·ªói x·ª≠ l√Ω t√†i li·ªáu: {str(e)}")
        return ""
    return text

def get_text_chunks(text):
    """Split text into chunks."""
    try:
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=800)
        chunks = text_splitter.split_text(text)
        return chunks
    except Exception as e:
        st.error(f"Error splitting text: {str(e)}")
        return []

def get_vector_store(text_chunks):
    """Create and save FAISS vector store."""
    try:
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)
        vector_store.save_local("faiss_index")
        st.success("T√†i li·ªáu ƒë√£ ph√¢n t√≠ch xong, s·∫µn s√†ng ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.")
    except Exception as e:
        st.error(f"Error creating vector store: {str(e)}")

def get_prompt_template():
    """Returns the prompt template."""
    prompt_template = """
    B·∫°n l√† m·ªôt chuy√™n vi√™n ph√¢n t√≠ch b√°o c√°o t√†i ch√≠nh chuy√™n nghi·ªáp, c√≥ nhi·ªám v·ª• h·ªó tr·ª£ ng∆∞·ªùi d√πng ph√¢n t√≠ch d·ªØ li·ªáu t√†i ch√≠nh doanh nghi·ªáp theo t·ª´ng nƒÉm. D·ªØ li·ªáu th∆∞·ªùng ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ b·∫£ng c√¢n ƒë·ªëi k·∫ø to√°n, bao g·ªìm c√°c ch·ªâ ti√™u sau:

    - T√†i s·∫£n ng·∫Øn h·∫°n (TSNH)
    - T·ªïng t√†i s·∫£n (TTS)
    - T·ªïng n·ª£ ph·∫£i tr·∫£ (T·ªïng N·ª£)
    - N·ª£ ng·∫Øn h·∫°n (N·ª£ NH)
    - V·ªën ch·ªß s·ªü h·ªØu (VCSH)

    Nhi·ªám v·ª• c·ªßa b·∫°n:

    - Ph√¢n t√≠ch c√°c s·ªë li·ªáu t√†i ch√≠nh do ng∆∞·ªùi d√πng cung c·∫•p, k·ªÉ c·∫£ theo t·ª´ng nƒÉm n·∫øu c√≥.
    - T√≠nh c√°c ch·ªâ s·ªë t√†i ch√≠nh c∆° b·∫£n v√† chuy√™n s√¢u, ∆∞u ti√™n s·ª≠ d·ª•ng c√¥ng th·ª©c do ng∆∞·ªùi d√πng cung c·∫•p n·∫øu c√≥. N·∫øu kh√¥ng, h√£y s·ª≠ d·ª•ng c√°c c√¥ng th·ª©c m·∫∑c ƒë·ªãnh sau:
        - T·ª∑ su·∫•t t√†i s·∫£n ng·∫Øn h·∫°n = TSNH / TTS
        - T·ª∑ l·ªá n·ª£ = T·ªïng N·ª£ / TTS
        - T·ª∑ l·ªá v·ªën ch·ªß s·ªü h·ªØu = VCSH / TTS
        - ƒê√≤n b·∫©y t√†i ch√≠nh (H·ªá s·ªë T√†i s·∫£n tr√™n V·ªën ch·ªß s·ªü h·ªØu) = TTS / VCSH
        - Kh·∫£ nƒÉng thanh to√°n hi·ªán h√†nh = TSNH / N·ª£ NH
        - V·ªën l∆∞u ƒë·ªông thu·∫ßn = TSNH - N·ª£ NH
        - TƒÉng tr∆∞·ªüng t√†i s·∫£n = (TTS nƒÉm sau ‚Äì TTS nƒÉm tr∆∞·ªõc) / TTS nƒÉm tr∆∞·ªõc
        - T·ª∑ l·ªá N·ª£ tr√™n V·ªën ch·ªß s·ªü h·ªØu = T·ªïng N·ª£ / VCSH
        - T√†i s·∫£n d√†i h·∫°n (ho·∫∑c T√†i s·∫£n c·ªë ƒë·ªãnh) = TTS - TSNH
        - N·ª£ d√†i h·∫°n = T·ªïng N·ª£ - N·ª£ NH
        - (v√† c√°c ch·ªâ s·ªë kh√°c c√≥ th·ªÉ suy ra t·ª´ d·ªØ li·ªáu ƒë√£ cho)
    - ƒê√°nh gi√° √Ω nghƒ©a t√†i ch√≠nh c·ªßa c√°c s·ªë li·ªáu v√† ch·ªâ s·ªë ƒë√£ t√≠nh to√°n, cung c·∫•p nh·∫≠n ƒë·ªãnh chuy√™n nghi·ªáp v√† b·ªëi c·∫£nh n·∫øu c√≥ th·ªÉ (v√≠ d·ª•: so s√°nh v·ªõi c√°c nƒÉm tr∆∞·ªõc, √Ω nghƒ©a t√≠ch c·ª±c/ti√™u c·ª±c).

    V√≠ d·ª• 1:
    User: Trong nƒÉm 2024, t√†i s·∫£n ng·∫Øn h·∫°n l√† 37.553 t·ª∑, t·ªïng t√†i s·∫£n l√† 55.049 t·ª∑. H√£y t√≠nh t·ª∑ su·∫•t t√†i s·∫£n ng·∫Øn h·∫°n v√† cho t√¥i bi·∫øt doanh nghi·ªáp c√≥ ·ªïn ƒë·ªãnh kh√¥ng?

    Assistant:
    T·ª∑ su·∫•t t√†i s·∫£n ng·∫Øn h·∫°n nƒÉm 2024 l√† 37.553 / 55.049 ‚âà 68,22%. ƒê√¢y l√† m·ªôt t·ª∑ l·ªá cao, cho th·∫•y doanh nghi·ªáp c√≥ kh·∫£ nƒÉng thanh to√°n ng·∫Øn h·∫°n r·∫•t t·ªët. N·∫øu so s√°nh v·ªõi c√°c nƒÉm tr∆∞·ªõc m√† v·∫´n gi·ªØ ·ªïn ƒë·ªãnh quanh m·ª©c n√†y th√¨ ƒë√¢y l√† m·ªôt d·∫•u hi·ªáu t√†i ch√≠nh t√≠ch c·ª±c.

    V√≠ d·ª• 2:
    User: T·ªïng n·ª£ nƒÉm 2024 l√† 19.500 t·ª∑, t·ªïng t√†i s·∫£n l√† 55.000 t·ª∑. H√£y ph√¢n t√≠ch t·ª∑ l·ªá n·ª£ gi√∫p t√¥i.

    Assistant:
    T·ª∑ l·ªá n·ª£ nƒÉm 2024 = 19.500 / 55.000 = 35,45%. T·ª∑ l·ªá n√†y ·ªü m·ª©c h·ª£p l√Ω. N·∫øu ph·∫ßn c√≤n l·∫°i ƒë∆∞·ª£c t√†i tr·ª£ b·ªüi v·ªën ch·ªß s·ªü h·ªØu th√¨ doanh nghi·ªáp ƒëang v·∫≠n h√†nh v·ªõi m·ª©c ƒë√≤n b·∫©y an to√†n.

    V√≠ d·ª• 3:
    User: NƒÉm 2023, t√†i s·∫£n ng·∫Øn h·∫°n l√† 35.900 t·ª∑, n·ª£ ng·∫Øn h·∫°n l√† 10.000 t·ª∑. T√¥i mu·ªën bi·∫øt v·ªën l∆∞u ƒë·ªông thu·∫ßn.

    Assistant:
    V·ªën l∆∞u ƒë·ªông thu·∫ßn = 35.900 ‚Äì 10.000 = 25.900 t·ª∑. ƒê√¢y l√† ph·∫ßn d∆∞ t√†i s·∫£n ng·∫Øn h·∫°n sau khi tr·ª´ ƒëi n·ª£ ng·∫Øn h·∫°n, th·ªÉ hi·ªán kh·∫£ nƒÉng duy tr√¨ ho·∫°t ƒë·ªông v√† d·ª± ph√≤ng ng·∫Øn h·∫°n c·ªßa doanh nghi·ªáp.

    V√≠ d·ª• 4:
    User: T·ªïng t√†i s·∫£n nƒÉm 2023 l√† 52.600 t·ª∑, nƒÉm 2024 l√† 55.000 t·ª∑. H√£y t√≠nh tƒÉng tr∆∞·ªüng t·ªïng t√†i s·∫£n.

    Assistant:
    TƒÉng tr∆∞·ªüng t·ªïng t√†i s·∫£n = (55.000 ‚Äì 52.600) / 52.600 = 4,57%. Doanh nghi·ªáp c√≥ m·ª©c tƒÉng tr∆∞·ªüng t·ªïng t√†i s·∫£n d∆∞∆°ng, cho th·∫•y quy m√¥ t√†i s·∫£n ti·∫øp t·ª•c ƒë∆∞·ª£c m·ªü r·ªông.

    V√≠ d·ª• 5 (M·ªõi - Minh h·ªça ∆∞u ti√™n c√¥ng th·ª©c ng∆∞·ªùi d√πng):
    User: T√†i s·∫£n ng·∫Øn h·∫°n nƒÉm 2023 l√† 30.000 t·ª∑, t·ªïng t√†i s·∫£n nƒÉm 2023 l√† 50.000 t·ª∑. H√£y t√≠nh T·ª∑ su·∫•t t√†i s·∫£n ng·∫Øn h·∫°n c·ªßa nƒÉm 2023, bi·∫øt r·∫±ng T·ª∑ su·∫•t t√†i s·∫£n ng·∫Øn h·∫°n = (t√†i s·∫£n ng·∫Øn h·∫°n / t·ªïng t√†i s·∫£n) * 100%.

    Assistant:
    Theo c√¥ng th·ª©c b·∫°n cung c·∫•p, T·ª∑ su·∫•t t√†i s·∫£n ng·∫Øn h·∫°n nƒÉm 2023 = (30.000 / 50.000) * 100% = 60%. ƒê√¢y l√† m·ªôt t·ª∑ l·ªá cao, cho th·∫•y doanh nghi·ªáp c√≥ c∆° c·∫•u t√†i s·∫£n ∆∞u ti√™n y·∫øu t·ªë thanh kho·∫£n ng·∫Øn h·∫°n.

    Ghi ch√∫:
    N·∫øu d·ªØ li·ªáu ng∆∞·ªùi d√πng ch∆∞a ƒë·∫ßy ƒë·ªß ƒë·ªÉ t√≠nh, h√£y h·ªèi l·∫°i th√¥ng tin c√≤n thi·∫øu m·ªôt c√°ch ng·∫Øn g·ªçn v√† g·ª£i √Ω ƒë√∫ng m√£ s·ªë c·∫ßn cung c·∫•p.

    Ng·ªØ c·∫£nh: {context}
    C√¢u h·ªèi: {question}

    Answer:
    """
    try:
        return PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    except Exception as e:
        st.error(f"Error creating prompt template: {str(e)}")
        return None

def user_input(user_question):
    """Process user question and return streamed response, integrated with chat history.""" 
    try:
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        
        if not os.path.exists("faiss_index"):
            st.error("Kh√¥ng t√¨m th·∫•y FAISS index. H√£y t·∫£i t√†i li·ªáu l√™n tr∆∞·ªõc.")
            return
        new_db = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
        docs = new_db.similarity_search(user_question)

        prompt_template_obj = get_prompt_template() # L·∫•y prompt template
        if not prompt_template_obj:
            return

        # N·ªëi n·ªôi dung c√°c t√†i li·ªáu t√¨m ƒë∆∞·ª£c v√†o ng·ªØ c·∫£nh
        context = "\n\n".join([doc.page_content for doc in docs])

        # T·∫°o prompt cu·ªëi c√πng ƒë√£ b·ªï sung ng·ªØ c·∫£nh RAG
        rag_augmented_question = prompt_template_obj.format(context=context, question=user_question)
        
        # Chuy·ªÉn ƒë·ªïi l·ªãch s·ª≠ chat c·ªßa Streamlit sang ƒë·ªãnh d·∫°ng c·ªßa Google Generative AI
        # Lo·∫°i b·ªè tin nh·∫Øn hi·ªán t·∫°i c·ªßa ng∆∞·ªùi d√πng kh·ªèi l·ªãch s·ª≠ ƒë·ªÉ tr√°nh tr√πng l·∫∑p khi g·ª≠i message
        gemini_history = []
        for msg in st.session_state.chat_history:
            if msg["role"] == "user":
                gemini_history.append({"role": "user", "parts": [msg["content"]]})
            elif msg["role"] == "assistant":
                gemini_history.append({"role": "model", "parts": [msg["content"]]})

        # Kh·ªüi t·∫°o model v√† chat session v·ªõi l·ªãch s·ª≠
        model = genai.GenerativeModel("gemini-2.0-flash")
        chat = model.start_chat(history=gemini_history)

        st.write("Reply:") 
        response_placeholder = st.empty()
        full_response_text = ""

        # Stream ph·∫£n h·ªìi t·ª´ m√¥ h√¨nh s·ª≠ d·ª•ng chat.send_message
        for chunk in chat.send_message(rag_augmented_question, stream=True):
            if chunk.text:
                full_response_text += chunk.text
                response_placeholder.markdown(full_response_text)
        
        # L∆∞u ph·∫£n h·ªìi c·ªßa bot v√†o session state v√† file
        st.session_state.chat_history.append({"role": "assistant", "content": full_response_text})
        save_chat_history(st.session_state.chat_history)

    except Exception as e:
        st.error(f"L·ªói x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}")

def main():
    st.set_page_config(page_title="Read Documents", page_icon="üìÑ")
    st.header("Demo ph√¢n t√≠ch t√†i li·ªáu üìÑ")

    # Kh·ªüi t·∫°o v√† t·∫£i l·ªãch s·ª≠ chat
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = load_chat_history()

    # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
    for message in st.session_state.chat_history:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    user_question = st.chat_input("B·∫°n h√£y h·ªèi sau khi t√†i li·ªáu ƒë√£ ƒë∆∞·ª£c ph√¢n t√≠ch xong")

    if user_question:
        st.session_state.chat_history.append({"role": "user", "content": user_question})
        with st.chat_message("user"):
            st.markdown(user_question)

        user_input(user_question)

    with st.sidebar:
        st.title("Menu")
        pdf_docs = st.file_uploader(
            "T·∫£i t√†i li·ªáu l√™n (PDF, TXT, XLSX, CSV, DOCX)",
            accept_multiple_files=True,
            type=["pdf", "docx", "txt", "xlsx", "csv"]
        )
        if st.button("Ph√¢n t√≠ch t√†i li·ªáu"):
            if not pdf_docs:
                st.error("Vui l√≤ng t·∫£i t√†i li·ªáu c·ªßa b·∫°n l√™n.")
                return
            with st.spinner("ƒêang x·ª≠ l√Ω..."):
                raw_text = get_text_from_documents(pdf_docs)
                if raw_text:
                    text_chunks = get_text_chunks(raw_text)
                    if text_chunks:
                        get_vector_store(text_chunks)
                    else:
                        st.error("Ki·ªÉm tra l·∫°i n·ªôi dung trong t√†i li·ªáu.")
                else:
                    st.error("Kh√¥ng c√≥ n·ªôi dung n√†o ƒë∆∞·ª£c trong t√†i li·ªáu.")
        
        # N√∫t x√≥a l·ªãch s·ª≠ chat
        if st.button("X√≥a l·ªãch s·ª≠ chat"):
            st.session_state.chat_history = []
            if os.path.exists("rag_chat_history.json"):
                os.remove("rag_chat_history.json")
            st.rerun()

if __name__ == "__main__":
    main()