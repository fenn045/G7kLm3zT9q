import streamlit as st
import pandas as pd
from docx import Document
from langchain_community.document_loaders import (
    PyPDFLoader, TextLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
import tempfile
from langchain_google_genai import GoogleGenerativeAIEmbeddings
import google.generativeai as genai
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
import json

# Load environment variables
load_dotenv()
api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    st.error("GOOGLE_API_KEY not found in environment variables.")
    st.stop()
genai.configure(api_key=api_key)

# H√†m t·∫£i l·ªãch s·ª≠ chat t·ª´ file
def load_chat_history():
    try:
        with open("rag_chat_history.json", "r", encoding="utf-8") as file:
            return json.load(file)
    except FileNotFoundError:
        return []

# H√†m l∆∞u l·ªãch s·ª≠ chat v√†o file
def save_chat_history(history):
    with open("rag_chat_history.json", "w", encoding="utf-8") as file:
        json.dump(history, file, ensure_ascii=False, indent=2)

def get_text_from_documents(docs):
    """Extract text from uploaded documents (PDF, TXT, XLSX, CSV, DOCX)."""
    text = ""
    try:
        for file in docs:
            suffix = os.path.splitext(file.name)[-1].lower()
            with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:
                tmp_file.write(file.read())
                tmp_path = tmp_file.name

            # Loaders theo lo·∫°i file
            if suffix == ".pdf":
                loader = PyPDFLoader(tmp_path)
                pages = loader.load_and_split()
                text += "\n".join([p.page_content for p in pages])

            elif suffix == ".txt":
                loader = TextLoader(tmp_path)
                pages = loader.load_and_split()
                text += "\n".join([p.page_content for p in pages])

            elif suffix == ".csv":
                df = pd.read_csv(tmp_path)
                text += df.to_string(index=False)

            elif suffix == ".xlsx":
                try:
                    sheets = pd.read_excel(tmp_path, sheet_name=None)
                    for name, df in sheets.items():
                        text += f"\n--- Sheet: {name} ---\n"
                        text += df.to_string(index=False)
                except Exception as e:
                    st.warning(f"L·ªói ƒë·ªçc Excel: {e}")
                    continue
                
            elif suffix == ".docx":
                try:
                    doc = Document(tmp_path)
                    for para in doc.paragraphs:
                        text += para.text + "\n"

                    for table in doc.tables:
                        for row in table.rows:
                            for cell in row.cells:
                                text += cell.text + " "
                        text += "\n" # Th√™m xu·ªëng d√≤ng sau m·ªói b·∫£ng
                        
                    text += "\n" # Th√™m xu·ªëng d√≤ng sau m·ªói file docx ƒë·ªÉ t√°ch bi·ªát
                except Exception as e:
                    st.warning(f"L·ªói ƒë·ªçc file DOCX: {e}")
                    continue
                
            else:
                st.warning(f"Kh√¥ng h·ªó tr·ª£ ƒë·ªãnh d·∫°ng file: {suffix}")
                continue

            os.unlink(tmp_path)

    except Exception as e:
        st.error(f"L·ªói x·ª≠ l√Ω t√†i li·ªáu: {str(e)}")
        return ""
    return text

def get_text_chunks(text):
    """Split text into chunks."""
    try:
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=800)
        chunks = text_splitter.split_text(text)
        return chunks
    except Exception as e:
        st.error(f"Error splitting text: {str(e)}")
        return []

def get_vector_store(text_chunks):
    """Create and save FAISS vector store."""
    try:
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)
        vector_store.save_local("faiss_index")
        st.success("T√†i li·ªáu ƒë√£ ph√¢n t√≠ch xong, s·∫µn s√†ng ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.")
    except Exception as e:
        st.error(f"Error creating vector store: {str(e)}")

def get_prompt_template():
    """Returns the prompt template."""
    prompt_template = """
    Tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch chi ti·∫øt nh·∫•t c√≥ th·ªÉ d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p. N·∫øu c√¢u tr·∫£ l·ªùi kh√¥ng c√≥ trong ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p, h√£y n√≥i, "C√¢u tr·∫£ l·ªùi kh√¥ng c√≥ trong ng·ªØ c·∫£nh."
    Kh√¥ng cung c·∫•p th√¥ng tin sai l·ªách.

    Ng·ªØ c·∫£nh: {context}
    C√¢u h·ªèi: {question}

    Answer:
    """
    try:
        return PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    except Exception as e:
        st.error(f"Error creating prompt template: {str(e)}")
        return None

def user_input(user_question):
    """Process user question and return streamed response, integrated with chat history.""" 
    try:
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        
        if not os.path.exists("faiss_index"):
            st.error("Kh√¥ng t√¨m th·∫•y FAISS index. H√£y t·∫£i t√†i li·ªáu l√™n tr∆∞·ªõc.")
            return
        new_db = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
        docs = new_db.similarity_search(user_question)

        prompt_template_obj = get_prompt_template() # L·∫•y prompt template
        if not prompt_template_obj:
            return

        # N·ªëi n·ªôi dung c√°c t√†i li·ªáu t√¨m ƒë∆∞·ª£c v√†o ng·ªØ c·∫£nh
        context = "\n\n".join([doc.page_content for doc in docs])

        # T·∫°o prompt cu·ªëi c√πng ƒë√£ b·ªï sung ng·ªØ c·∫£nh RAG
        rag_augmented_question = prompt_template_obj.format(context=context, question=user_question)
        
        # Chuy·ªÉn ƒë·ªïi l·ªãch s·ª≠ chat c·ªßa Streamlit sang ƒë·ªãnh d·∫°ng c·ªßa Google Generative AI
        # Lo·∫°i b·ªè tin nh·∫Øn hi·ªán t·∫°i c·ªßa ng∆∞·ªùi d√πng kh·ªèi l·ªãch s·ª≠ ƒë·ªÉ tr√°nh tr√πng l·∫∑p khi g·ª≠i message
        gemini_history = []
        for msg in st.session_state.chat_history:
            if msg["role"] == "user":
                gemini_history.append({"role": "user", "parts": [msg["content"]]})
            elif msg["role"] == "assistant":
                gemini_history.append({"role": "model", "parts": [msg["content"]]})

        # Kh·ªüi t·∫°o model v√† chat session v·ªõi l·ªãch s·ª≠
        model = genai.GenerativeModel("gemini-2.0-flash")
        chat = model.start_chat(history=gemini_history)

        st.write("Reply:") 
        response_placeholder = st.empty()
        full_response_text = ""

        # Stream ph·∫£n h·ªìi t·ª´ m√¥ h√¨nh s·ª≠ d·ª•ng chat.send_message
        for chunk in chat.send_message(rag_augmented_question, stream=True):
            if chunk.text:
                full_response_text += chunk.text
                response_placeholder.markdown(full_response_text)
        
        # L∆∞u ph·∫£n h·ªìi c·ªßa bot v√†o session state v√† file
        st.session_state.chat_history.append({"role": "assistant", "content": full_response_text})
        save_chat_history(st.session_state.chat_history)

    except Exception as e:
        st.error(f"L·ªói x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}")

def main():
    st.set_page_config(page_title="Read Documents", page_icon="üìÑ")
    st.header("Demo ph√¢n t√≠ch t√†i li·ªáu üìÑ")

    # Kh·ªüi t·∫°o v√† t·∫£i l·ªãch s·ª≠ chat
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = load_chat_history()

    # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
    for message in st.session_state.chat_history:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    user_question = st.chat_input("B·∫°n h√£y h·ªèi sau khi t√†i li·ªáu ƒë√£ ƒë∆∞·ª£c ph√¢n t√≠ch xong")

    if user_question:
        st.session_state.chat_history.append({"role": "user", "content": user_question})
        with st.chat_message("user"):
            st.markdown(user_question)

        user_input(user_question)

    with st.sidebar:
        st.title("Menu")
        pdf_docs = st.file_uploader(
            "T·∫£i t√†i li·ªáu l√™n (PDF, TXT, XLSX, CSV, DOCX)",
            accept_multiple_files=True,
            type=["pdf", "docx", "txt", "xlsx", "csv"]
        )
        if st.button("Ph√¢n t√≠ch t√†i li·ªáu"):
            if not pdf_docs:
                st.error("Vui l√≤ng t·∫£i t√†i li·ªáu c·ªßa b·∫°n l√™n.")
                return
            with st.spinner("ƒêang x·ª≠ l√Ω..."):
                raw_text = get_text_from_documents(pdf_docs)
                if raw_text:
                    text_chunks = get_text_chunks(raw_text)
                    if text_chunks:
                        get_vector_store(text_chunks)
                    else:
                        st.error("Ki·ªÉm tra l·∫°i n·ªôi dung trong t√†i li·ªáu.")
                else:
                    st.error("Kh√¥ng c√≥ n·ªôi dung n√†o ƒë∆∞·ª£c trong t√†i li·ªáu.")
        
        # N√∫t x√≥a l·ªãch s·ª≠ chat
        if st.button("X√≥a l·ªãch s·ª≠ chat"):
            st.session_state.chat_history = []
            if os.path.exists("rag_chat_history.json"):
                os.remove("rag_chat_history.json")
            st.rerun()

if __name__ == "__main__":
    main()