import streamlit as st
import pandas as pd
from docx import Document
from langchain_community.document_loaders import (
    PyPDFLoader, TextLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
import tempfile
from langchain_google_genai import GoogleGenerativeAIEmbeddings
import google.generativeai as genai
from langchain_community.vectorstores import FAISS
from langchain_google_genai import ChatGoogleGenerativeAI # V·∫´n c·∫ßn ƒë·ªÉ l·∫•y prompt template object
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
import json # B·ªï sung: Import th∆∞ vi·ªán json ƒë·ªÉ x·ª≠ l√Ω file history #

# Load environment variables
load_dotenv()
api_key = os.getenv("GOOGLE_API_KEY")
if not api_key:
    st.error("GOOGLE_API_KEY not found in environment variables.")
    st.stop()
genai.configure(api_key=api_key)

# B·ªï sung: H√†m t·∫£i l·ªãch s·ª≠ chat t·ª´ file #
def load_chat_history(): #
    try: #
        with open("rag_chat_history.json", "r", encoding="utf-8") as file: #
            return json.load(file) #
    except FileNotFoundError: #
        return [] #
# K·∫øt th√∫c b·ªï sung #

# B·ªï sung: H√†m l∆∞u l·ªãch s·ª≠ chat v√†o file #
def save_chat_history(history): #
    with open("rag_chat_history.json", "w", encoding="utf-8") as file: #
        json.dump(history, file, ensure_ascii=False, indent=2) #
# K·∫øt th√∫c b·ªï sung #

def get_text_from_documents(docs):
    """Extract text from uploaded documents (PDF, TXT, XLSX, CSV, DOCX)."""
    text = ""
    try:
        for file in docs:
            suffix = os.path.splitext(file.name)[-1].lower()
            with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:
                tmp_file.write(file.read())
                tmp_path = tmp_file.name

            # Loaders theo lo·∫°i file
            if suffix == ".pdf":
                loader = PyPDFLoader(tmp_path)
                pages = loader.load_and_split()
                text += "\n".join([p.page_content for p in pages])

            elif suffix == ".txt":
                loader = TextLoader(tmp_path)
                pages = loader.load_and_split()
                text += "\n".join([p.page_content for p in pages])

            elif suffix == ".csv":
                df = pd.read_csv(tmp_path)
                text += df.to_string(index=False)

            elif suffix == ".xlsx":
                try:
                    sheets = pd.read_excel(tmp_path, sheet_name=None)
                    for name, df in sheets.items():
                        text += f"\n--- Sheet: {name} ---\n"
                        text += df.to_string(index=False)
                except Exception as e:
                    st.warning(f"L·ªói ƒë·ªçc Excel: {e}")
                    continue
                
            elif suffix == ".docx":
                try:
                    doc = Document(tmp_path)
                    for para in doc.paragraphs:
                        text += para.text + "\n"
                    # C√≥ th·ªÉ b·ªï sung ƒë·ªçc text t·ª´ b·∫£ng (tables) n·∫øu c·∫ßn #
                    for table in doc.tables:
                        for row in table.rows:
                            for cell in row.cells:
                                text += cell.text + " "
                        text += "\n" # Th√™m xu·ªëng d√≤ng sau m·ªói b·∫£ng #
                    text += "\n" # Th√™m xu·ªëng d√≤ng sau m·ªói file docx ƒë·ªÉ t√°ch bi·ªát #
                except Exception as e:
                    st.warning(f"L·ªói ƒë·ªçc file DOCX: {e}")
                    continue
                
            else:
                st.warning(f"Kh√¥ng h·ªó tr·ª£ ƒë·ªãnh d·∫°ng file: {suffix}")
                continue

            os.unlink(tmp_path)

    except Exception as e:
        st.error(f"L·ªói x·ª≠ l√Ω t√†i li·ªáu: {str(e)}")
        return ""
    return text

def get_text_chunks(text):
    """Split text into chunks."""
    try:
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=800)
        chunks = text_splitter.split_text(text)
        return chunks
    except Exception as e:
        st.error(f"Error splitting text: {str(e)}")
        return []

def get_vector_store(text_chunks):
    """Create and save FAISS vector store."""
    try:
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)
        vector_store.save_local("faiss_index")
        st.success("T√†i li·ªáu ƒë√£ ph√¢n t√≠ch xong, s·∫µn s√†ng ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi.")
    except Exception as e:
        st.error(f"Error creating vector store: {str(e)}")

# ƒê√£ thay ƒë·ªïi h√†m n√†y ƒë·ªÉ tr·∫£ v·ªÅ prompt template (kh√¥ng c·∫ßn model ·ªü ƒë√¢y n·ªØa) #
def get_prompt_template(): # ƒê√É THAY ƒê·ªîI T√äN H√ÄM V√Ä TR·∫¢ V·ªÄ #
    """Returns the prompt template."""
    prompt_template = """
    Tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch chi ti·∫øt nh·∫•t c√≥ th·ªÉ d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p. N·∫øu c√¢u tr·∫£ l·ªùi kh√¥ng c√≥ trong ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p, h√£y n√≥i, "C√¢u tr·∫£ l·ªùi kh√¥ng c√≥ trong ng·ªØ c·∫£nh."
    Kh√¥ng cung c·∫•p th√¥ng tin sai l·ªách.

    Ng·ªØ c·∫£nh: {context}
    C√¢u h·ªèi: {question}

    Answer:
    """
    try:
        return PromptTemplate(template=prompt_template, input_variables=["context", "question"]) #
    except Exception as e:
        st.error(f"Error creating prompt template: {str(e)}") # ƒê√É ƒê·ªîI TH√îNG B√ÅO L·ªñI #
        return None

def user_input(user_question):
    """Process user question and return streamed response, integrated with chat history.""" # ƒê√É C·∫¨P NH·∫¨T M√î T·∫¢ #
    try:
        embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
        
        if not os.path.exists("faiss_index"):
            st.error("Kh√¥ng t√¨m th·∫•y FAISS index. H√£y t·∫£i t√†i li·ªáu l√™n tr∆∞·ªõc.")
            return
        new_db = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
        docs = new_db.similarity_search(user_question)

        prompt_template_obj = get_prompt_template() # L·∫•y prompt template #
        if not prompt_template_obj: #
            return #

        # N·ªëi n·ªôi dung c√°c t√†i li·ªáu t√¨m ƒë∆∞·ª£c v√†o ng·ªØ c·∫£nh #
        context = "\n\n".join([doc.page_content for doc in docs])

        # T·∫°o prompt cu·ªëi c√πng ƒë√£ b·ªï sung ng·ªØ c·∫£nh RAG #
        rag_augmented_question = prompt_template_obj.format(context=context, question=user_question) # ƒê√É THAY ƒê·ªîI #
        
        # B·ªï sung: Chuy·ªÉn ƒë·ªïi l·ªãch s·ª≠ chat c·ªßa Streamlit sang ƒë·ªãnh d·∫°ng c·ªßa Google Generative AI #
        # Lo·∫°i b·ªè tin nh·∫Øn hi·ªán t·∫°i c·ªßa ng∆∞·ªùi d√πng kh·ªèi l·ªãch s·ª≠ ƒë·ªÉ tr√°nh tr√πng l·∫∑p khi g·ª≠i message #
        gemini_history = []
        for msg in st.session_state.chat_history:
            if msg["role"] == "user":
                gemini_history.append({"role": "user", "parts": [msg["content"]]})
            elif msg["role"] == "assistant":
                gemini_history.append({"role": "model", "parts": [msg["content"]]})
        # K·∫øt th√∫c b·ªï sung #

        # B·ªï sung: Kh·ªüi t·∫°o model v√† chat session v·ªõi l·ªãch s·ª≠ #
        model = genai.GenerativeModel("gemini-2.0-flash") #
        chat = model.start_chat(history=gemini_history) #
        # K·∫øt th√∫c b·ªï sung #

        st.write("Reply:") 
        response_placeholder = st.empty()
        full_response_text = ""

        # B·ªï sung: Stream ph·∫£n h·ªìi t·ª´ m√¥ h√¨nh s·ª≠ d·ª•ng chat.send_message #
        for chunk in chat.send_message(rag_augmented_question, stream=True): #
            if chunk.text: #
                full_response_text += chunk.text #
                response_placeholder.markdown(full_response_text)
                # T√πy ch·ªçn: Th√™m m·ªôt ƒë·ªô tr·ªÖ nh·ªè ƒë·ªÉ t·∫°o hi·ªáu ·ª©ng g√µ ph√≠m r√µ r√†ng h∆°n
        # K·∫øt th√∫c b·ªï sung #
        
        # B·ªï sung: L∆∞u ph·∫£n h·ªìi c·ªßa bot v√†o session state v√† file #
        st.session_state.chat_history.append({"role": "assistant", "content": full_response_text}) #
        save_chat_history(st.session_state.chat_history) #
        # K·∫øt th√∫c b·ªï sung #

    except Exception as e:
        st.error(f"L·ªói x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}")

def main():
    st.set_page_config(page_title="Chat with Documents", page_icon="üìÑ")
    st.header("Demo Chatbot ph√¢n t√≠ch t√†i li·ªáu")

    # B·ªï sung: Kh·ªüi t·∫°o v√† t·∫£i l·ªãch s·ª≠ chat #
    if "chat_history" not in st.session_state: #
        st.session_state.chat_history = load_chat_history() #
    # K·∫øt th√∫c b·ªï sung #

    # B·ªï sung: Hi·ªÉn th·ªã l·ªãch s·ª≠ chat #
    for message in st.session_state.chat_history: #
        with st.chat_message(message["role"]): #
            st.markdown(message["content"]) #
    # K·∫øt th√∫c b·ªï sung #
    
    # B·ªï sung: S·ª≠ d·ª•ng st.chat_input thay v√¨ st.text_input #
    user_question = st.chat_input("B·∫°n h√£y h·ªèi sau khi t√†i li·ªáu ƒë√£ ƒë∆∞·ª£c ph√¢n t√≠ch xong") #
    # K·∫øt th√∫c b·ªï sung #

    if user_question:
        # B·ªï sung: L∆∞u c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng v√†o l·ªãch s·ª≠ tr∆∞·ªõc khi x·ª≠ l√Ω #
        st.session_state.chat_history.append({"role": "user", "content": user_question}) #
        # Hi·ªÉn th·ªã c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng ngay l·∫≠p t·ª©c #
        with st.chat_message("user"): #
            st.markdown(user_question) #
        # K·∫øt th√∫c b·ªï sung #

        user_input(user_question) # H√†m user_input gi·ªù s·∫Ω t·ª± l∆∞u bot response

    with st.sidebar:
        st.title("Menu")
        pdf_docs = st.file_uploader(
            "T·∫£i t√†i li·ªáu l√™n (PDF, TXT, XLSX, CSV, DOCX)",
            accept_multiple_files=True,
            type=["pdf", "docx", "txt", "xlsx", "csv"]
        )
        if st.button("Ph√¢n t√≠ch t√†i li·ªáu"):
            if not pdf_docs:
                st.error("Vui l√≤ng t·∫£i t√†i li·ªáu c·ªßa b·∫°n l√™n.")
                return
            with st.spinner("ƒêang x·ª≠ l√Ω..."):
                raw_text = get_text_from_documents(pdf_docs)
                if raw_text:
                    text_chunks = get_text_chunks(raw_text)
                    if text_chunks:
                        get_vector_store(text_chunks)
                    else:
                        st.error("Ki·ªÉm tra l·∫°i n·ªôi dung trong t√†i li·ªáu.")
                else:
                    st.error("Kh√¥ng c√≥ n·ªôi dung n√†o ƒë∆∞·ª£c trong t√†i li·ªáu.")
        
        # B·ªï sung: N√∫t x√≥a l·ªãch s·ª≠ chat #
        if st.button("X√≥a l·ªãch s·ª≠ chat"): #
            st.session_state.chat_history = [] #
            if os.path.exists("rag_chat_history.json"): #
                os.remove("rag_chat_history.json") #
            st.rerun() #
        # K·∫øt th√∫c b·ªï sung #

if __name__ == "__main__":
    main()